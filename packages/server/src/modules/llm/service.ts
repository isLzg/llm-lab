import { GoogleGenAI } from "@google/genai";
import OpenAI from "openai";
import { LLMModel } from "./model";

const gemini = new GoogleGenAI({
  apiKey: process.env.GEMINI_API_KEY,
});

const deepseek = new OpenAI({
  baseURL: process.env.DEEPSEEK_BASE_URL,
  apiKey: process.env.DEEPSEEK_API_KEY,
});

export const LLMService = {
  // Gemini API
  async generateContentWithGemini(
    body: typeof LLMModel.generateContentBody.static
  ): Promise<typeof LLMModel.generateContentResponse.static> {
    console.log("ü§ñ Gemini ~ body:", body);

    try {
      const response = await gemini.models.generateContent({
        model: body.model || "gemini-2.5-flash",
        contents: body.contents,
      });

      const text = response.text || "";
      console.log("ü§ñ Gemini ~ response:", text);

      return {
        text,
      };
    } catch (error) {
      console.error("‚ùå Gemini API error:", error);
      throw error;
    }
  },

  // DeepSeek API
  async generateContentWithDeepSeek(
    body: typeof LLMModel.generateContentBody.static
  ): Promise<typeof LLMModel.generateContentResponse.static> {
    console.log("ü§ñ DeepSeek ~ body:", body);

    try {
      const completion = await deepseek.chat.completions.create({
        messages: [{ role: "user", content: body.contents }],
        model: body.model || "deepseek-chat",
      });

      const response = completion.choices[0].message.content;
      console.log("ü§ñ DeepSeek ~ response:", response);

      return {
        text: response || "",
      };
    } catch (error) {
      console.error("‚ùå DeepSeek API error:", error);
      throw error;
    }
  },
};
